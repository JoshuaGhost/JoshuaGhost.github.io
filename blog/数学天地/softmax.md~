Title: 也谈softmax（施工中）
Date:  2017-02-22
Tags:  数学

##引言
随着机器学习的异常火爆，学习它的人也如雨后春笋一般冒了出来，作为时刻时代前沿的Ghost自然也没能免俗跟着一起跳进了大坑，今天不说别的，就着青梅煮酒，我胡扯一点关于机器学习中极为重要的一个函数：softmax

##softmax是什么？
softmax是一个用在分类问题中，描述一个输入集预测某个分类概率大小的一个函数，要谈softmax，我们先来和它混个脸熟：
$$
P(y=i|\vec{x})=\frac{e^{{x}^{\, T} \vec{w_i}}}{\sum_{j=1}^{M}e^{\vec{x}^{\, T} \vec{w_j}}}
$$
公式左边的$P(y=i|\vec{x})$表示“在我的输入数据是$\vec{x}$的时候，把这个数据分到第$i$类中的概率”; \\
右边的式子比较吓人，咱们一点一点来分析。

先看分子，注意分子上$\vec{w}$的下标和预测的y，我们不妨称呼分子为“当前数据点分到i分类时的适合程度”，简称“i适配度”。分子的式子本身由于是自然指数函数，是对$x$单调的，也就是说它相当于把原来相对“均匀”地分布在$\vec{x}$所在空间中的各个数据点分配了一个“权重”，$\vec{x}^{\, T}\vec{w_i}$越大的项“权重”也就越大，而w则是用来线性调整$\vec{x}$的“权重”。到这里可能有人会问，我们已经有一个线性权重了，为什么还需要再套一层幂指数呢？由于一方面仅仅使用线性的权重，难以拟合一些比较复杂的函数;另一方面由于人类认知能力本来就是非线性的，例如给出两组数据，0.6与0.8; 0.7与0.9，与1的距离进行比较，如果是我的话会觉得0.6、0.8一组距离1都“差着不少”，但0.9就与0.7迥然不同不同，已经“非常接近”1了。而要让计算机理解这种感觉，我们可以巧妙地使用$e^0.6$、$e^0.8$和$e$比较，再用$e^0.7$、$e^0.9$和$e$比较，这样一来就可以直接使用数值差的绝对值来表示这种“感觉”。

再看分母，分母这个形式有个特定名称叫做“配分函数（Partition function）”，话说这个奇怪的名字我一直觉得就是为了符合音译而硬出来的…………它的作用是遍历加和所有$M$个分类的“适配度”，看看分子所对应的第i个分类到底能不能在所有可能分类中脱颖而出。并且分母相当于一个正则化因子，它让所有的$P$都能被真正视作一个“概率”，即$\forall i: 0<P(y=i|\vec{x})<1$并且$\sum_{i=1}^{M}P(y=i|\vec{x})=1$
##softmax的来历
softmax函数源自物理系四大神课之《热力学与统计物理》（另外三门传说是《经典力学》《量子场论》和《电动力学》，排名不分先后）。里面有一种分布叫做Boltzman distribution，又名Gibbs distribution，长这样：
$$
P(\epsilon_i)=\frac{e^{-\frac{\epsilon_i}{k_b T}}}{\sum_{j}e^{-\frac{\epsilon}{k_b T}}}
$$
其中：

$\epsilon_i$表示单个单元的能量，或指系综（系统的集合）能量的一种分布方式;

$k_b$是玻尔兹曼常数，$k_b=1.38064852(79)\times 10^{-23}JK^{-1}$;

$T$是热力学温标。

Boltzman分布有两种用途，一是用来描述在一个正则系综（即与大热池等温的若干系统的集合，系统之间没有粒子与能量交换）中，系统能量的分布为$\epsilon_i$的概率; 或者另一种用途，是描述一个等温体系下，测量出能量为$\epsilon_i$粒子的概率

